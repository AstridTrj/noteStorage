{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"D:/PythonProject/didi_giscup/data/giscup_2021/\"\n",
    "train_path = r\"processed_train\"\n",
    "test_path = r\"20200901/\"\n",
    "weather_path = root_path + \"weather.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_weather():\n",
    "weather = pd.read_csv(weather_path)\n",
    "we_change = {\"cloudy\": 1, \"moderate rain\": 2, \"showers\": 3, \"heavy rain\": 4, \"rainstorm\": 5}\n",
    "weather[\"weather\"] = weather[\"weather\"].map(we_change)\n",
    "weather.index = weather[\"date\"]\n",
    "all_link = []\n",
    "\n",
    "\n",
    "# add feature link_list and write to file\n",
    "def modify_head(day_path: str):\n",
    "    head = pd.read_csv(day_path + \"head.csv\")\n",
    "    if \"link_list\" in head.columns:\n",
    "        display(day_path + \"head.csv was already modified.\")\n",
    "        return \n",
    "    link = pd.read_csv(day_path + \"link.csv\")\n",
    "    link[\"link_id\"] = link[\"link_id\"].astype(np.int).astype(np.str)\n",
    "    link_id = link.groupby(\"order_id\").apply(lambda x: x[\"link_id\"].values.tolist())\n",
    "    head = pd.merge(head, link_id.rename(\"link_list\"), left_on=\"order_id\", right_index=True)\n",
    "    head.to_csv(day_path + \"head.csv\", index=False)\n",
    "    display(day_path + \"head.csv write successful.\")\n",
    "\n",
    "hid_size = 128\n",
    "def train_w2v_model():\n",
    "    path_ = root_path + \"processed_train/\"\n",
    "    day_list = list(range(1, 32))\n",
    "    if 3 in day_list:\n",
    "        day_list.remove(3)\n",
    "    # add trianset link_id\n",
    "    for d in day_list:\n",
    "        new_file_name = path_ + \"202008\" + \"{:0>2d}\".format(d) + \"/\"\n",
    "        head = pd.read_csv(new_file_name + \"head.csv\")\n",
    "        global all_link\n",
    "        all_link.extend(head[\"link_list\"].values.tolist())\n",
    "        all_link = list(set(all_link))\n",
    "        print(d, \"'s link added.\")\n",
    "    \n",
    "    # add testset link_id\n",
    "    head = pd.read_csv(root_path + test_path + \"head.csv\")\n",
    "    all_link.extend(head[\"link_list\"].values.tolist())\n",
    "    del head\n",
    "    all_link = list(set(all_link))\n",
    "    print(\"Test's link added.\")\n",
    "    \n",
    "    all_link = [ast.literal_eval(lk) for lk in tqdm(all_link)]\n",
    "    w2v_model = Word2Vec(all_link, vector_size=hid_size)\n",
    "    w2v_model.save(\"./w2v_model.model\")\n",
    "    print(\"wordd2vec model training finish.\")\n",
    "    \n",
    "\n",
    "def built_vec():\n",
    "    w2v_model = Word2Vec.load(\"./w2v_model.model\")\n",
    "    list_vec = []\n",
    "    for idx in tqdm(w2v_model.wv.index_to_key):\n",
    "        temp = [int(idx)]\n",
    "        temp.extend(list(w2v_model.wv[idx]))\n",
    "        list_vec.append(temp)\n",
    "    link_col = [\"vec_dim_\" + str(i) for i in range(hid_size)]\n",
    "    link_col.insert(0, \"link_id\")\n",
    "    wv_df = pd.DataFrame(list_vec, columns=link_col)\n",
    "    wv_df.to_csv(\"w2v_table.csv\", index=False)    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def day_feature(day_path: str):\n",
    "    head = pd.read_csv(day_path + \"head.csv\")\n",
    "    link = pd.read_csv(day_path + \"link.csv\")\n",
    "    cross = pd.read_csv(day_path + \"cross.csv\")\n",
    "#     print(\"start link shape: \", link.shape)\n",
    "#     display(\"head: \", head.head())\n",
    "    \n",
    "    # weather set up\n",
    "    time_id = int(day_path[-9:-1])\n",
    "    head[\"weather\"] = weather.loc[time_id, \"weather\"]\n",
    "    head[\"hightemp\"] = weather.loc[time_id, \"hightemp\"]\n",
    "    head[\"lowtemp\"] = weather.loc[time_id, \"lowtemp\"]\n",
    "    head[\"temp_sub\"] = head[\"hightemp\"] - head[\"lowtemp\"]\n",
    "\n",
    "    # slice id features\n",
    "    head[\"slice_id\"] = head[\"slice_id\"].astype(int)\n",
    "    head[\"slice_1m\"] = head[\"slice_id\"] * 5\n",
    "    head[\"slice_30m\"] = (head[\"slice_id\"] * 5) // 30\n",
    "    head[\"slice_1h\"] = (head[\"slice_id\"] * 5) // 60\n",
    "    \n",
    "    # link count\n",
    "    link_cnt = link[\"order_id\"].value_counts()\n",
    "    head = pd.merge(head, link_cnt.rename(\"link_cnt\"), left_on=\"order_id\", right_index=True)\n",
    "    \n",
    "    # mean distance\n",
    "    head[\"mean_distance\"] = head[\"distance\"] / head[\"link_cnt\"]\n",
    "    head[\"speed_one\"] = head[\"distance\"] / head[\"simple_eta\"]\n",
    "    \n",
    "    \n",
    "    # link_time features\n",
    "    link_statics = link.groupby(\"order_id\")[\"link_time\"].agg(link_time_sum=\"sum\", link_time_mean=\"mean\",\n",
    "                                                            link_time_max=\"max\", link_time_min=\"min\")\n",
    "    head = pd.merge(head, link_statics, left_on=\"order_id\", right_index=True)\n",
    "    head[\"speed_two\"] = head[\"distance\"] / head[\"link_time_sum\"]\n",
    "    # link_current_status features\n",
    "    current_status = link.groupby(\"order_id\")[\"link_current_status\"].agg(link_cur_sta_mean=\"mean\",\n",
    "                                                                        link_cur_sta_sum=\"sum\")\n",
    "    head = pd.merge(head, current_status, left_on=\"order_id\", right_index=True)\n",
    "    conges = link[link.link_current_status > 2].groupby(\"order_id\")[\"link_current_status\"].agg(conges_cnt=\"count\",\n",
    "                                                                                          conges_sum=\"sum\")\n",
    "    head = pd.merge(head, conges, on=\"order_id\", how=\"left\")\n",
    "    head.fillna(0, inplace=True)\n",
    "    \n",
    "    amble = link[link.link_current_status == 2].groupby(\"order_id\")[\"link_current_status\"].agg(amble_cnt=\"count\",\n",
    "                                                                                          amble_sum=\"sum\")\n",
    "    head = pd.merge(head, amble, on=\"order_id\", how=\"left\")\n",
    "    head.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    # cross count\n",
    "    cross_cnt = cross[\"order_id\"].value_counts()\n",
    "    head = pd.merge(head, cross_cnt.rename(\"cross_cnt\"), left_on=\"order_id\", right_index=True)\n",
    "    # cross_time features\n",
    "    cross_statics = cross.groupby(\"order_id\")[\"cross_time\"].agg(cross_time_sum=\"sum\", cross_time_mean=\"mean\",\n",
    "                                                                cross_time_max=\"max\", cross_time_mode=\"median\")\n",
    "    head = pd.merge(head, cross_statics, left_on=\"order_id\", right_index=True)\n",
    "    \n",
    "    first_cross = cross.drop_duplicates(subset=[\"order_id\"], keep=\"first\")[[\"order_id\", \"cross_from\", \"cross_to\"]]\n",
    "    head = pd.merge(head, first_cross, on=\"order_id\", suffixes=(\"\", \"_first\"))\n",
    "    last_cross = cross.drop_duplicates(subset=[\"order_id\"], keep=\"last\")[[\"order_id\", \"cross_from\", \"cross_to\"]]\n",
    "    head = pd.merge(head, last_cross, on=\"order_id\", suffixes=(\"\", \"_last\"))\n",
    "    \n",
    "    head[\"link_time_sum_ratio\"] = head[\"link_time_sum\"] / head[\"simple_eta\"]\n",
    "    \n",
    "#     display(head.columns)\n",
    "#     display(\"processed head: \", head.head(7))\n",
    "    head.to_csv(day_path + \"feature.csv\", index=False)\n",
    "\n",
    "# modify head file\n",
    "def modify_main():\n",
    "    path_ = root_path + \"processed_train/\"\n",
    "    day_list = list(range(1, 32))\n",
    "    if 3 in day_list:\n",
    "        day_list.remove(3)\n",
    "    for d in day_list:\n",
    "        new_file_name = \"202008\" + \"{:0>2d}\".format(d) + \"/\"\n",
    "        modify_head(path_ + new_file_name)\n",
    "    modify_head(root_path + test_path)\n",
    "\n",
    "    \n",
    "def feature_main():\n",
    "    path_ = root_path + \"processed_train/\"\n",
    "    day_list = list(range(1, 2))\n",
    "    if 3 in day_list:\n",
    "        day_list.remove(3)\n",
    "    for d in day_list:\n",
    "        new_file_name = \"202008\" + \"{:0>2d}\".format(d) + \"/\"\n",
    "        modify_head(path_ + new_file_name)\n",
    "#         day_feature(path_ + new_file_name)\n",
    "#         print(new_file_name, \"feature built sucessfully.\")\n",
    "#     day_feature(root_path + test_path)\n",
    "#     print(\"20200901/ feature built sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 's link added.\n",
      "2 's link added.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a2bfcf3e86d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# feature_main()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# modify_main()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_w2v_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mbuilt_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3928f928bff0>\u001b[0m in \u001b[0;36mtrain_w2v_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mday_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mnew_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"202008\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"{:0>2d}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mhead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_file_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"head.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0mall_link\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mall_link\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"link_list\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \"\"\"\n\u001b[0;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# feature_main()\n",
    "# modify_main()\n",
    "train_w2v_model()\n",
    "built_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1, 1, 3]\n",
    "# b = [2, 4, 2]\n",
    "# c = [1, 1, 2]\n",
    "# d = pd.DataFrame({\"a\": a, \"b\": b, \"c\": c})\n",
    "# display(d)\n",
    "# def func(gro):\n",
    "#     gro[\"d\"] = gro[\"b\"] + 1\n",
    "#     gro[\"e\"] = gro[\"c\"] + 1\n",
    "#     display(gro)\n",
    "#     return gro\n",
    "# d = d.groupby(\"a\").apply(func)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(set([1, 4, 5]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.loc[20200801, \"hightemp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
