{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"D:/PythonProject/didi_giscup/data/giscup_2021/\"\n",
    "train_path = r\"processed_train\"\n",
    "test_path = r\"20200901/\"\n",
    "weather_path = root_path + \"weather.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_weather():\n",
    "weather = pd.read_csv(weather_path)\n",
    "we_change = {\"cloudy\": 1, \"moderate rain\": 2, \"showers\": 3, \"heavy rain\": 4, \"rainstorm\": 5}\n",
    "weather[\"weather\"] = weather[\"weather\"].map(we_change)\n",
    "weather.index = weather[\"date\"]\n",
    "all_link = []\n",
    "\n",
    "\n",
    "# add feature link_list and write to file\n",
    "def modify_head(day_path: str):\n",
    "    head = pd.read_csv(day_path + \"head.csv\")\n",
    "    if \"link_list\" in head.columns:\n",
    "        display(day_path + \"head.csv was already modified.\")\n",
    "        return \n",
    "    link = pd.read_csv(day_path + \"link.csv\")\n",
    "    link[\"link_id\"] = link[\"link_id\"].astype(np.int).astype(np.str)\n",
    "    link_id = link.groupby(\"order_id\").apply(lambda x: x[\"link_id\"].values.tolist())\n",
    "    head = pd.merge(head, link_id.rename(\"link_list\"), left_on=\"order_id\", right_index=True)\n",
    "    head.to_csv(day_path + \"head.csv\", index=False)\n",
    "    display(day_path + \"head.csv write successful.\")\n",
    "\n",
    "hid_size = 128\n",
    "def train_w2v_model():\n",
    "    path_ = root_path + \"processed_train/\"\n",
    "    day_list = list(range(1, 32))\n",
    "    if 3 in day_list:\n",
    "        day_list.remove(3)\n",
    "    # add trianset link_id\n",
    "    for d in day_list:\n",
    "        new_file_name = path_ + \"202008\" + \"{:0>2d}\".format(d) + \"/\"\n",
    "        head = pd.read_csv(new_file_name + \"head.csv\")\n",
    "        global all_link\n",
    "        all_link.extend(head[\"link_list\"].values.tolist())\n",
    "        all_link = list(set(all_link))\n",
    "        print(d, \"'s link added.\")\n",
    "    \n",
    "    # add testset link_id\n",
    "    head = pd.read_csv(root_path + test_path + \"head.csv\")\n",
    "    all_link.extend(head[\"link_list\"].values.tolist())\n",
    "    del head\n",
    "    all_link = list(set(all_link))\n",
    "    print(\"Test's link added.\")\n",
    "    \n",
    "    all_link = [ast.literal_eval(lk) for lk in tqdm(all_link)]\n",
    "    w2v_model = Word2Vec(all_link, vector_size=hid_size)\n",
    "    w2v_model.save(\"./w2v_model.model\")\n",
    "    print(\"wordd2vec model training finish.\")\n",
    "    \n",
    "\n",
    "def built_vec():\n",
    "    w2v_model = Word2Vec.load(\"./w2v_model.model\")\n",
    "    list_vec = []\n",
    "    for idx in tqdm(w2v_model.wv.index_to_key):\n",
    "        temp = [int(idx)]\n",
    "        temp.extend(list(w2v_model.wv[idx]))\n",
    "        list_vec.append(temp)\n",
    "    link_col = [\"vec_dim_\" + str(i) for i in range(hid_size)]\n",
    "    link_col.insert(0, \"link_id\")\n",
    "    wv_df = pd.DataFrame(list_vec, columns=link_col)\n",
    "    wv_df.to_csv(\"w2v_table.csv\", index=False)    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def day_feature(day_path: str):\n",
    "    head = pd.read_csv(day_path + \"head.csv\")\n",
    "    link = pd.read_csv(day_path + \"link.csv\")\n",
    "    cross = pd.read_csv(day_path + \"cross.csv\")\n",
    "#     print(\"start link shape: \", link.shape)\n",
    "#     display(\"head: \", head.head())\n",
    "    \n",
    "    # weather set up\n",
    "    time_id = int(day_path[-9:-1])\n",
    "    head[\"weather\"] = weather.loc[time_id, \"weather\"]\n",
    "    head[\"hightemp\"] = weather.loc[time_id, \"hightemp\"]\n",
    "    head[\"lowtemp\"] = weather.loc[time_id, \"lowtemp\"]\n",
    "    head[\"temp_sub\"] = head[\"hightemp\"] - head[\"lowtemp\"]\n",
    "\n",
    "    # slice id features\n",
    "    head[\"slice_id\"] = head[\"slice_id\"].astype(int)\n",
    "    head[\"slice_1m\"] = head[\"slice_id\"] * 5\n",
    "    head[\"slice_30m\"] = (head[\"slice_id\"] * 5) // 30\n",
    "    head[\"slice_1h\"] = (head[\"slice_id\"] * 5) // 60\n",
    "    \n",
    "    # link count\n",
    "    link_cnt = link[\"order_id\"].value_counts()\n",
    "    head = pd.merge(head, link_cnt.rename(\"link_cnt\"), left_on=\"order_id\", right_index=True)\n",
    "    \n",
    "    # mean distance\n",
    "    head[\"mean_distance\"] = head[\"distance\"] / head[\"link_cnt\"]\n",
    "    head[\"speed_one\"] = head[\"distance\"] / head[\"simple_eta\"]\n",
    "    \n",
    "    \n",
    "    # link_time features\n",
    "    link_statics = link.groupby(\"order_id\")[\"link_time\"].agg(link_time_sum=\"sum\", link_time_mean=\"mean\",\n",
    "                                                            link_time_max=\"max\", link_time_min=\"min\")\n",
    "    head = pd.merge(head, link_statics, left_on=\"order_id\", right_index=True)\n",
    "    head[\"speed_two\"] = head[\"distance\"] / head[\"link_time_sum\"]\n",
    "    # link_current_status features\n",
    "    current_status = link.groupby(\"order_id\")[\"link_current_status\"].agg(link_cur_sta_mean=\"mean\",\n",
    "                                                                        link_cur_sta_sum=\"sum\")\n",
    "    head = pd.merge(head, current_status, left_on=\"order_id\", right_index=True)\n",
    "    conges = link[link.link_current_status > 2].groupby(\"order_id\")[\"link_current_status\"].agg(conges_cnt=\"count\",\n",
    "                                                                                          conges_sum=\"sum\")\n",
    "    head = pd.merge(head, conges, on=\"order_id\", how=\"left\")\n",
    "    head.fillna(0, inplace=True)\n",
    "    \n",
    "    amble = link[link.link_current_status == 2].groupby(\"order_id\")[\"link_current_status\"].agg(amble_cnt=\"count\",\n",
    "                                                                                          amble_sum=\"sum\")\n",
    "    head = pd.merge(head, amble, on=\"order_id\", how=\"left\")\n",
    "    head.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    # cross count\n",
    "    cross_cnt = cross[\"order_id\"].value_counts()\n",
    "    head = pd.merge(head, cross_cnt.rename(\"cross_cnt\"), left_on=\"order_id\", right_index=True)\n",
    "    # cross_time features\n",
    "    cross_statics = cross.groupby(\"order_id\")[\"cross_time\"].agg(cross_time_sum=\"sum\", cross_time_mean=\"mean\",\n",
    "                                                                cross_time_max=\"max\", cross_time_mode=\"median\")\n",
    "    head = pd.merge(head, cross_statics, left_on=\"order_id\", right_index=True)\n",
    "    \n",
    "    first_cross = cross.drop_duplicates(subset=[\"order_id\"], keep=\"first\")[[\"order_id\", \"cross_from\", \"cross_to\"]]\n",
    "    head = pd.merge(head, first_cross, on=\"order_id\", suffixes=(\"\", \"_first\"))\n",
    "    last_cross = cross.drop_duplicates(subset=[\"order_id\"], keep=\"last\")[[\"order_id\", \"cross_from\", \"cross_to\"]]\n",
    "    head = pd.merge(head, last_cross, on=\"order_id\", suffixes=(\"\", \"_last\"))\n",
    "    \n",
    "    head[\"link_time_sum_ratio\"] = head[\"link_time_sum\"] / head[\"simple_eta\"]\n",
    "    \n",
    "#     display(head.columns)\n",
    "#     display(\"processed head: \", head.head(7))\n",
    "    head.to_csv(day_path + \"feature.csv\", index=False)\n",
    "\n",
    "# modify head file\n",
    "def modify_main():\n",
    "    path_ = root_path + \"processed_train/\"\n",
    "    day_list = list(range(1, 32))\n",
    "    if 3 in day_list:\n",
    "        day_list.remove(3)\n",
    "    for d in day_list:\n",
    "        new_file_name = \"202008\" + \"{:0>2d}\".format(d) + \"/\"\n",
    "        modify_head(path_ + new_file_name)\n",
    "    modify_head(root_path + test_path)\n",
    "\n",
    "    \n",
    "def feature_main():\n",
    "    path_ = root_path + \"processed_train/\"\n",
    "    day_list = list(range(1, 2))\n",
    "    if 3 in day_list:\n",
    "        day_list.remove(3)\n",
    "    for d in day_list:\n",
    "        new_file_name = \"202008\" + \"{:0>2d}\".format(d) + \"/\"\n",
    "        modify_head(path_ + new_file_name)\n",
    "#         day_feature(path_ + new_file_name)\n",
    "#         print(new_file_name, \"feature built sucessfully.\")\n",
    "#     day_feature(root_path + test_path)\n",
    "#     print(\"20200901/ feature built sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 's link added.\n",
      "2 's link added.\n",
      "4 's link added.\n",
      "5 's link added.\n",
      "6 's link added.\n",
      "7 's link added.\n",
      "8 's link added.\n",
      "9 's link added.\n",
      "10 's link added.\n",
      "11 's link added.\n",
      "12 's link added.\n",
      "13 's link added.\n",
      "14 's link added.\n",
      "15 's link added.\n",
      "16 's link added.\n",
      "17 's link added.\n",
      "18 's link added.\n",
      "19 's link added.\n",
      "20 's link added.\n",
      "21 's link added.\n",
      "22 's link added.\n",
      "23 's link added.\n",
      "24 's link added.\n",
      "25 's link added.\n",
      "26 's link added.\n",
      "27 's link added.\n",
      "28 's link added.\n",
      "29 's link added.\n",
      "30 's link added.\n",
      "31 's link added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                      | 0/7653290 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test's link added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████▋                                                    | 1928889/7653290 [41:16<22:08, 4309.81it/s]"
     ]
    }
   ],
   "source": [
    "# feature_main()\n",
    "# modify_main()\n",
    "train_w2v_model()\n",
    "built_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1, 1, 3]\n",
    "# b = [2, 4, 2]\n",
    "# c = [1, 1, 2]\n",
    "# d = pd.DataFrame({\"a\": a, \"b\": b, \"c\": c})\n",
    "# display(d)\n",
    "# def func(gro):\n",
    "#     gro[\"d\"] = gro[\"b\"] + 1\n",
    "#     gro[\"e\"] = gro[\"c\"] + 1\n",
    "#     display(gro)\n",
    "#     return gro\n",
    "# d = d.groupby(\"a\").apply(func)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(set([1, 4, 5]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.loc[20200801, \"hightemp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
